{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4nGu8qIwyZzRTdfmmQ1ba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zslsally/CUDA-Learn-Notes/blob/main/elementwise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbhMi-r0arte",
        "outputId": "2a254ec5-7fc2-46ba-f3a1-096370ab3482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 16 22:32:22 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yIU8m_Za64m",
        "outputId": "f3394f33-bb8c-4d8d-e210-2550851f489a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export TORCH_EXTENSIONS_DIR=/content/torch_extensions"
      ],
      "metadata": {
        "id": "691JeXCkFWtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhR9NTe3ZMot",
        "outputId": "4ba344cd-2b59-47ec-ffbb-be130c6bd1d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "import time\n",
        "from torch.utils.cpp_extension import load\n",
        "from typing import Optional\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlhzi28vZRFT",
        "outputId": "56811021-0af2-46b4-ce2b-ae988a0bfcc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x79be1dcdd690>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CUDA kernel as a python module\n",
        "lib = load(name='elementwise_lib',\n",
        "           sources=['elementwise.cu'],\n",
        "           extra_cuda_cflags=[\n",
        "               \"-O3\",\n",
        "                \"-U__CUDA_NO_HALF_OPERATORS__\",\n",
        "                \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n",
        "                \"-U__CUDA_NO_HALF2_OPERATORS__\",\n",
        "                \"-U__CUDA_NO_BFLOAT16_CONVERSIONS__\",\n",
        "                \"--expt-relaxed-constexpr\",\n",
        "                \"--expt-extended-lambda\",\n",
        "                \"--use_fast_math\",\n",
        "            ],\n",
        "           extra_cflags=['-std=c++17'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMQ5qC_wZap_",
        "outputId": "766b0eb1-d902-4844-9ece-ed34384d2b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_benchmark(perf_func: callable, a: torch.Tensor, b: torch.Tensor, tag: str,\n",
        "                  out: Optional[torch.Tensor] = None, warmup: int = 10,\n",
        "                  iters: int = 1000, show_all: bool = False):\n",
        "    # torch.dot vs custom dot_prod kernel\n",
        "    if out is not None:\n",
        "        out.fill_(0)\n",
        "    # warmup\n",
        "    if out is not None:\n",
        "        for i in range(warmup):\n",
        "            perf_func(a, b, out)\n",
        "    else:\n",
        "        for i in range(warmup):\n",
        "            _ = perf_func(a, b)\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    # iters\n",
        "    if out is not None:\n",
        "        for i in range(iters):\n",
        "            perf_func(a, b, out)\n",
        "    else:\n",
        "        for i in range(iters):\n",
        "            out = perf_func(a, b)\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "    total_time = (end - start) * 1000 # ms\n",
        "    mean_time = total_time / iters\n",
        "    out_info = f\"out_{tag}\"\n",
        "    out_val = out.flatten().detach().cpu().numpy().tolist()[:2]\n",
        "    out_val = [round(v, 8) for v in out_val]\n",
        "    print(f\"{out_info:>18}: {out_val}, time:{mean_time:.8f}ms\")\n",
        "    if show_all: print(out)\n",
        "    return out, mean_time"
      ],
      "metadata": {
        "id": "IWG9KwUWZbGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ss = [1024, 2048, 4096]\n",
        "Ks = [1024, 2048, 4096]\n",
        "SKs = [(S, K) for S in Ss for K in Ks]\n",
        "\n",
        "for (S, K) in SKs:\n",
        "    print(\"-\" * 85)\n",
        "    print(\" \" * 40 + f\"S={S}, K={K}\")\n",
        "    a = torch.randn((S, K)).cuda().float().contiguous()\n",
        "    b = torch.randn((S, K)).cuda().float().contiguous()\n",
        "    c = torch.zeros_like(a).cuda().float().contiguous()\n",
        "    run_benchmark(lib.elementwise_add_f32,   a, b, \"f32\",   c)\n",
        "    run_benchmark(lib.elementwise_add_f32x4, a, b, \"f32x4\", c)\n",
        "    run_benchmark(partial(torch.add, out=c), a, b, \"f32_th\")\n",
        "\n",
        "    print(\"-\" * 85)\n",
        "    a_f16 = a.half().contiguous()\n",
        "    b_f16 = b.half().contiguous()\n",
        "    c_f16 = c.half().contiguous()\n",
        "    run_benchmark(lib.elementwise_add_f16,        a_f16, b_f16, \"f16\",       c_f16)\n",
        "    run_benchmark(lib.elementwise_add_f16x2,      a_f16, b_f16, \"f16x2\",     c_f16)\n",
        "    run_benchmark(lib.elementwise_add_f16x8,      a_f16, b_f16, \"f16x8\",     c_f16)\n",
        "    run_benchmark(lib.elementwise_add_f16x8_pack, a_f16, b_f16, \"f16x8pack\", c_f16)\n",
        "    run_benchmark(partial(torch.add, out=c_f16),  a_f16, b_f16, \"f16_th\")\n",
        "    print(\"-\" * 85)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSZdcmkiGdUa",
        "outputId": "aeb3aba1-d81a-495d-ae75-401ffa22d93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------------\n",
            "                                        S=1024, K=1024\n",
            "           out_f32: [1.30998158, -1.69877255], time:0.06985998ms\n",
            "         out_f32x4: [1.30998158, -1.69877255], time:0.05388236ms\n",
            "        out_f32_th: [1.30998158, -1.69877255], time:0.05390501ms\n",
            "-------------------------------------------------------------------------------------\n",
            "           out_f16: [1.31054688, -1.69921875], time:0.04510260ms\n",
            "         out_f16x2: [1.31054688, -1.69921875], time:0.02726936ms\n",
            "         out_f16x8: [1.31054688, -1.69921875], time:0.03176880ms\n",
            "     out_f16x8pack: [1.31054688, -1.69921875], time:0.02609944ms\n",
            "        out_f16_th: [1.31054688, -1.69921875], time:0.02635026ms\n",
            "-------------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------------\n",
            "                                        S=1024, K=2048\n",
            "           out_f32: [0.02171495, -0.87937462], time:0.09853315ms\n",
            "         out_f32x4: [0.02171495, -0.87937462], time:0.10464954ms\n",
            "        out_f32_th: [0.02171495, -0.87937462], time:0.10637188ms\n",
            "-------------------------------------------------------------------------------------\n",
            "           out_f16: [0.02171326, -0.87890625], time:0.05248260ms\n",
            "         out_f16x2: [0.02171326, -0.87890625], time:0.05122066ms\n",
            "         out_f16x8: [0.02171326, -0.87890625], time:0.06557536ms\n",
            "     out_f16x8pack: [0.02171326, -0.87890625], time:0.05470896ms\n",
            "        out_f16_th: [0.02171326, -0.87890625], time:0.05279946ms\n",
            "-------------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------------\n",
            "                                        S=1024, K=4096\n",
            "           out_f32: [0.27876574, 1.07925105], time:0.19709063ms\n",
            "         out_f32x4: [0.27876574, 1.07925105], time:0.20794320ms\n",
            "        out_f32_th: [0.27876574, 1.07925105], time:0.20849824ms\n",
            "-------------------------------------------------------------------------------------\n",
            "           out_f16: [0.27856445, 1.07910156], time:0.10228753ms\n",
            "         out_f16x2: [0.27856445, 1.07910156], time:0.09990048ms\n",
            "         out_f16x8: [0.27856445, 1.07910156], time:0.13242388ms\n",
            "     out_f16x8pack: [0.27856445, 1.07910156], time:0.10585642ms\n",
            "        out_f16_th: [0.27856445, 1.07910156], time:0.10147762ms\n",
            "-------------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------------\n",
            "                                        S=2048, K=1024\n",
            "           out_f32: [-1.47156096, -1.84401894], time:0.10027385ms\n",
            "         out_f32x4: [-1.47156096, -1.84401894], time:0.10617089ms\n",
            "        out_f32_th: [-1.47156096, -1.84401894], time:0.10618758ms\n",
            "-------------------------------------------------------------------------------------\n",
            "           out_f16: [-1.47167969, -1.84375], time:0.05286145ms\n",
            "         out_f16x2: [-1.47167969, -1.84375], time:0.05077004ms\n",
            "         out_f16x8: [-1.47167969, -1.84375], time:0.06285572ms\n",
            "     out_f16x8pack: [-1.47167969, -1.84375], time:0.05382895ms\n",
            "        out_f16_th: [-1.47167969, -1.84375], time:0.05285382ms\n",
            "-------------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------------\n",
            "                                        S=2048, K=2048\n",
            "           out_f32: [-3.07089067, 2.52044868], time:0.19746614ms\n",
            "         out_f32x4: [-3.07089067, 2.52044868], time:0.20845318ms\n",
            "        out_f32_th: [-3.07089067, 2.52044868], time:0.20933890ms\n",
            "-------------------------------------------------------------------------------------\n",
            "           out_f16: [-3.0703125, 2.52148438], time:0.10318756ms\n",
            "         out_f16x2: [-3.0703125, 2.52148438], time:0.10037780ms\n",
            "         out_f16x8: [-3.0703125, 2.52148438], time:0.13058853ms\n",
            "     out_f16x8pack: [-3.0703125, 2.52148438], time:0.10664392ms\n",
            "        out_f16_th: [-3.0703125, 2.52148438], time:0.10276055ms\n",
            "-------------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------------\n",
            "                                        S=2048, K=4096\n",
            "           out_f32: [1.28746819, -2.25272894], time:0.39203715ms\n",
            "         out_f32x4: [1.28746819, -2.25272894], time:0.41485739ms\n",
            "        out_f32_th: [1.28746819, -2.25272894], time:0.41767359ms\n",
            "-------------------------------------------------------------------------------------\n",
            "           out_f16: [1.28710938, -2.25390625], time:0.20352507ms\n",
            "         out_f16x2: [1.28710938, -2.25390625], time:0.19782352ms\n",
            "         out_f16x8: [1.28710938, -2.25390625], time:0.26446557ms\n",
            "     out_f16x8pack: [1.28710938, -2.25390625], time:0.20799541ms\n",
            "        out_f16_th: [1.28710938, -2.25390625], time:0.20101810ms\n",
            "-------------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------------\n",
            "                                        S=4096, K=1024\n",
            "           out_f32: [1.12943912, 0.67204666], time:0.19771504ms\n",
            "         out_f32x4: [1.12943912, 0.67204666], time:0.20916963ms\n",
            "        out_f32_th: [1.12943912, 0.67204666], time:0.20978546ms\n",
            "-------------------------------------------------------------------------------------\n",
            "           out_f16: [1.12988281, 0.67236328], time:0.10550165ms\n",
            "         out_f16x2: [1.12988281, 0.67236328], time:0.10017276ms\n",
            "         out_f16x8: [1.12988281, 0.67236328], time:0.12434459ms\n",
            "     out_f16x8pack: [1.12988281, 0.67236328], time:0.10634160ms\n",
            "        out_f16_th: [1.12988281, 0.67236328], time:0.10189128ms\n",
            "-------------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------------\n",
            "                                        S=4096, K=2048\n",
            "           out_f32: [-0.05611712, -1.58681417], time:0.39257860ms\n",
            "         out_f32x4: [-0.05611712, -1.58681417], time:0.41479850ms\n",
            "        out_f32_th: [-0.05611712, -1.58681417], time:0.41559601ms\n",
            "-------------------------------------------------------------------------------------\n",
            "           out_f16: [-0.05615234, -1.58691406], time:0.20371938ms\n",
            "         out_f16x2: [-0.05615234, -1.58691406], time:0.19954538ms\n",
            "         out_f16x8: [-0.05615234, -1.58691406], time:0.26135540ms\n",
            "     out_f16x8pack: [-0.05615234, -1.58691406], time:0.20962763ms\n",
            "        out_f16_th: [-0.05615234, -1.58691406], time:0.20181727ms\n",
            "-------------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------------\n",
            "                                        S=4096, K=4096\n",
            "           out_f32: [-0.17691848, 0.21774538], time:0.78424025ms\n",
            "         out_f32x4: [-0.17691848, 0.21774538], time:0.82564592ms\n",
            "        out_f32_th: [-0.17691848, 0.21774538], time:0.82656050ms\n",
            "-------------------------------------------------------------------------------------\n",
            "           out_f16: [-0.17700195, 0.21765137], time:0.40361333ms\n",
            "         out_f16x2: [-0.17700195, 0.21765137], time:0.39332080ms\n",
            "         out_f16x8: [-0.17700195, 0.21765137], time:0.52770805ms\n",
            "     out_f16x8pack: [-0.17700195, 0.21765137], time:0.41341615ms\n",
            "        out_f16_th: [-0.17700195, 0.21765137], time:0.39755177ms\n",
            "-------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}